{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0638fda-b97b-4857-8d59-5ef5c7045728",
   "metadata": {},
   "source": [
    "# The Ninapro Dataset\n",
    "[Can be found here](https://ninapro.hevs.ch/)\n",
    "\n",
    "In this week we used the Ninapro dataset DB1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f2e4c-25e7-4feb-8d53-90a8dcf0d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "rel_dir = 'datasets/ninapro/db1/s1/S1_A1_E1.mat'\n",
    "file_path = os.path.join(root_dir, rel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac3bc1-0db5-4a97-970f-e0d9a6d47448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat as ld\n",
    "\n",
    "mat_data = ld(rel_dir, appendmat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79f7d0-4928-4553-957d-2884bd37ac6d",
   "metadata": {},
   "source": [
    "## Available fields in the ninapro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b3f6d-2c88-4e18-aa6a-41a9021f1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(mat_data.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955d030-7091-423f-9555-3c04a041c09b",
   "metadata": {},
   "source": [
    "## Loading all the subjects' data into a 'struct'.\n",
    "This struct 'data', will have the following structure:\n",
    "\n",
    "Data is an array of n subjects\n",
    "each subject containing 3 experiments\n",
    "each experiment containing a dictionary (key-value pair)\n",
    "we'll load the sEMG signals from all the channels, and the cyberglove data, for the hand joint angle regression task.\n",
    "\n",
    "We also computed the angle differences between samples, to model the correlation between joint angle variation and the incoming EMG signal. This variation per sample is given by the following equation:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta_i\\left[n\\right] &= \\begin{cases}\n",
    "        0 &\\quad,\\text{if } n \\leq 1\\\\\n",
    "        g_i\\left[n+1\\right]-g_i\\left[n\\right]\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Where $i$ is a cyberglove joint ($i \\in \\left[1,22\\right], i \\in \\mathbb{R}$), $\\delta\\left[n\\right]$ is the angle variation (in $\\degree$) at sample $n$, $g\\left[n\\right]$ is the cyberglove's uncalibrated angle at sample $n$.\n",
    "\n",
    "## Normalize data\n",
    "The emg signal was normalized from each channel's relative minimum and max, to $\\left[0,1\\right]$\n",
    "while the glove angles were normalized, from $\\left[0,360\\right]$ to $\\left[0,1\\right]$, using:\n",
    "\n",
    "\n",
    "$$x' = \\frac{x - \\text{min}~x}{\\text{max}~x - \\text{min}~x}$$\n",
    "\n",
    "And 'delta', was normalized from $\\left[-360,360\\right]$ to $\\left[-1,1\\right]$, using:\n",
    "\n",
    "$$x'' = 2 \\frac{x - \\text{min}~x}{\\text{max}~x - \\text{min}~x}$$\n",
    "\n",
    "(source: https://stats.stackexchange.com/questions/178626/how-to-normalize-data-between-1-and-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d63ac0-6572-4a6f-b6b3-74405e3a2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Define the structure array to store the data\n",
    "num_subjects = 27\n",
    "data = []\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "\n",
    "def normalizer(data_array,min_val,max_val,type):\n",
    "    \n",
    "    match type:\n",
    "        case \"uni\": \n",
    "            return (data_array - min_val) / (max_val - min_val)            \n",
    "        case \"bi\":\n",
    "            return 2 * ( (data_array - min_val) / (max_val - min_val ) ) - 1\n",
    "        case __:\n",
    "            raise Exception(f\"{type} type normalization not yet implemented.\")\n",
    "\n",
    "# Loop through each subject\n",
    "    # Initialize lists to hold the experimental data\n",
    "\n",
    "emg_data = []\n",
    "delta_data = []\n",
    "\n",
    "for subj_idx in range(1, num_subjects + 1):\n",
    "    # Set up the subject directory name and path\n",
    "    subject_dir = f\"s{subj_idx}\"\n",
    "    subject_path = os.path.join(dataset_path, subject_dir)\n",
    "\n",
    "\n",
    "    # Loop through each experiment for the current subject\n",
    "    for exp_idx in range(1, 4):\n",
    "        # Set up the experiment file name and path\n",
    "        exp_name = f\"S{subj_idx}_A1_E{exp_idx}.mat\"\n",
    "        exp_path = os.path.join(subject_path, exp_name)\n",
    "\n",
    "        # Load the MATLAB file\n",
    "        mat_file = loadmat(exp_path)\n",
    "\n",
    "        # Extract the relevant information from the loaded dictionary\n",
    "        emg = mat_file['emg']\n",
    "        glove = mat_file['glove']\n",
    "        \n",
    "        # glove delta\n",
    "        delta = np.diff(glove, axis=0) # compute differences\n",
    "        delta = np.pad(delta, ((1, 0), (0, 0)), mode='constant') # first sample delta = 0\n",
    "\n",
    "        # normalize data\n",
    "        norm_emg = normalizer(emg, np.min(emg, axis = 0), np.max(emg, axis = 0) , \"uni\") # min-max channel, [0;1]\n",
    "        #norm_glove = normalizer(glove,0,360, \"uni\") # min-max channel, [0;1]\n",
    "        norm_delta = normalizer(delta,-360,360, \"bi\")\n",
    "\n",
    "        emg_data.append(norm_emg)\n",
    "        delta_data.append(norm_delta)\n",
    "        \n",
    "        # Append the list of experiments to the data list\n",
    "emg_data = np.array(emg_data, dtype=object)\n",
    "delta_data = np.array(delta_data, dtype=object)\n",
    "\n",
    "np.savez_compressed(os.path.join(dataset_path, \"db_1_data.npz\"), emg=emg_data, delta=delta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca55d6c-6e4f-4985-b649-30c2c2b516d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "We now have the EMG signals to use as input to the network, and the dependent variable for regression, the $\\delta\\left[n\\right]$. This should suffice for the finger regression task, however, we'll\n",
    "try different approaches, such as:\n",
    "- Different network architectures: Vanilla RNN, LSTM, Bi-directional LSTM\n",
    "- Use of covariates, such as the physiological data of the subject (sex, age, height, weight, available in the ninapro dataset DB1), to try to\n",
    "improve inter-subject accuracy.\n",
    "- Use of amputated patient data from the Ninapro dataset.\n",
    "\n",
    "The next step is to break down the data into batches, and then try training a RNN with differnt input sequence lenghts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f6369-0622-4928-97fd-c252bd82faf6",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8ec789e-d6f9-45da-bb83-014a68a81e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62721, 10, 200)\n",
      "(62721, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "window_size = 200 # splits all datapoints into 200 samples\n",
    "\n",
    "def transpose_array(array):\n",
    "    for idx in range(len(array)):\n",
    "        array[idx] = np.transpose(array[idx])\n",
    "    array_t = array\n",
    "    return array_t\n",
    "\n",
    "# load processed file\n",
    "# put samplesXchannel into channelXsamples\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "data_load_path = os.path.join(dataset_path,\"db_1_data.npz\")\n",
    "with np.load(data_load_path, allow_pickle=True) as data:\n",
    "    emg_data = transpose_array(data['emg']) \n",
    "    delta_data = transpose_array(data['delta'])\n",
    "\n",
    "def segment_data(emg_data, delta_data, window_size):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    num_trials = emg_data.shape[0]\n",
    "    for trial_idx in range(num_trials):\n",
    "        # Calculate the largest index that is a multiple of window_size\n",
    "        max_idx = (emg_data[trial_idx].shape[1] // window_size) * window_size\n",
    "        #print(f'Trial {trial_idx}: data size = {emg_data[trial_idx].shape[1]}, max_idx = {max_idx}')\n",
    "        for start_idx in range(0, max_idx, window_size):\n",
    "            input_t = emg_data[trial_idx][:,start_idx:start_idx + window_size]\n",
    "            target_t = delta_data[trial_idx][:,start_idx:start_idx + 1]\n",
    "            inputs.append(input_t)\n",
    "            targets.append(target_t)\n",
    "\n",
    "    inputs = np.stack(inputs, axis=0)\n",
    "    targets = np.stack(targets, axis=0)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "X, Y = segment_data(emg_data, delta_data, window_size)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "#dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "#filename = os.path.join(dataset_path,'segmented_data.npz')\n",
    "#np.savez_compressed(filename, inputs=X, targets=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de888714-f9b2-484a-ae1c-b1d8f74cd302",
   "metadata": {},
   "source": [
    "# TODO amanhã\n",
    "\n",
    "Tentar treinar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83293837-51be-4083-92a5-3eb269d4960f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e0ccd50-d447-40d6-aedb-1ce0cfa045aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model properties\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Multivariate LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "471f08e0-ffde-4aaa-b9b9-bb9e654b11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "filename = os.path.join(dataset_path,'segmented_data.npz')\n",
    "\n",
    "with np.load(filename) as data:\n",
    "    X = data['inputs']\n",
    "    Y = data['targets']\n",
    "\n",
    "X = torch.from_numpy(X).to(torch.float32).to(device)\n",
    "Y = torch.from_numpy(Y).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f4d565c-81b1-446e-a152-cf28b24d6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).to(torch.float32).to(device)\n",
    "Y = torch.from_numpy(Y).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f2d99e46-d6ac-4895-84b0-55bb7c4d29b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.444336809683591e-06\n",
      "Epoch 2/100, Loss: 2.279440423080814e-06\n",
      "Epoch 3/100, Loss: 8.677125151734799e-08\n",
      "Epoch 4/100, Loss: 2.2819439493559912e-07\n",
      "Epoch 5/100, Loss: 8.526488386451092e-07\n",
      "Epoch 6/100, Loss: 6.93563748654924e-08\n",
      "Epoch 7/100, Loss: 1.1324183191163684e-07\n",
      "Epoch 8/100, Loss: 1.2727048215310788e-07\n",
      "Epoch 9/100, Loss: 9.878334594759508e-07\n",
      "Epoch 10/100, Loss: 5.3693632651174994e-08\n",
      "Epoch 11/100, Loss: 1.002454064291669e-05\n",
      "Epoch 12/100, Loss: 2.4172257084842386e-08\n",
      "Epoch 13/100, Loss: 1.0848589226952754e-06\n",
      "Epoch 14/100, Loss: 8.108859219646547e-07\n",
      "Epoch 15/100, Loss: 2.7670806090895894e-08\n",
      "Epoch 16/100, Loss: 3.112176827357871e-08\n",
      "Epoch 17/100, Loss: 1.0932136973451634e-07\n",
      "Epoch 18/100, Loss: 2.7543022085296798e-08\n",
      "Epoch 19/100, Loss: 5.0288917918805964e-08\n",
      "Epoch 20/100, Loss: 3.3074069705207876e-08\n",
      "Epoch 21/100, Loss: 1.205842323770412e-07\n",
      "Epoch 22/100, Loss: 2.624556714181381e-07\n",
      "Epoch 23/100, Loss: 2.9529328315902603e-08\n",
      "Epoch 24/100, Loss: 2.081364073092118e-06\n",
      "Epoch 25/100, Loss: 3.0450128178927116e-06\n",
      "Epoch 26/100, Loss: 5.337785324854849e-08\n",
      "Epoch 27/100, Loss: 1.4122238667368947e-07\n",
      "Epoch 28/100, Loss: 7.084617550390249e-07\n",
      "Epoch 29/100, Loss: 5.2734023370248906e-08\n",
      "Epoch 30/100, Loss: 4.809538154404436e-07\n",
      "Epoch 31/100, Loss: 2.4599685275461525e-06\n",
      "Epoch 32/100, Loss: 2.0666283262471552e-07\n",
      "Epoch 33/100, Loss: 7.794732681531968e-08\n",
      "Epoch 34/100, Loss: 5.673369329883826e-08\n",
      "Epoch 35/100, Loss: 2.5446762919045796e-08\n",
      "Epoch 36/100, Loss: 3.037748399492557e-07\n",
      "Epoch 37/100, Loss: 8.208149182564739e-08\n",
      "Epoch 38/100, Loss: 3.005843041137268e-08\n",
      "Epoch 39/100, Loss: 5.3415895706621086e-08\n",
      "Epoch 40/100, Loss: 6.027111254525153e-08\n",
      "Epoch 41/100, Loss: 5.55058932150132e-08\n",
      "Epoch 42/100, Loss: 4.2381476106356786e-08\n",
      "Epoch 43/100, Loss: 6.081540959712584e-08\n",
      "Epoch 44/100, Loss: 1.0300367137006106e-07\n",
      "Epoch 45/100, Loss: 3.9875825308399726e-08\n",
      "Epoch 46/100, Loss: 3.5460404887999175e-06\n",
      "Epoch 47/100, Loss: 7.080390673763759e-08\n",
      "Epoch 48/100, Loss: 3.241673951492885e-08\n",
      "Epoch 49/100, Loss: 2.8727825451824174e-07\n",
      "Epoch 50/100, Loss: 2.1309645603651006e-08\n",
      "Epoch 51/100, Loss: 1.761878287709351e-08\n",
      "Epoch 52/100, Loss: 1.6549834072066005e-06\n",
      "Epoch 53/100, Loss: 1.466037019781652e-06\n",
      "Epoch 54/100, Loss: 5.772385947011571e-08\n",
      "Epoch 55/100, Loss: 1.1186857662437433e-08\n",
      "Epoch 56/100, Loss: 2.7697049986841193e-08\n",
      "Epoch 57/100, Loss: 3.151122427880182e-06\n",
      "Epoch 58/100, Loss: 4.9274781588337646e-08\n",
      "Epoch 59/100, Loss: 2.254708775240033e-08\n",
      "Epoch 60/100, Loss: 6.82957974618148e-08\n",
      "Epoch 61/100, Loss: 1.4131243730730603e-08\n",
      "Epoch 62/100, Loss: 8.00494944996899e-07\n",
      "Epoch 63/100, Loss: 3.092403133564403e-08\n",
      "Epoch 64/100, Loss: 2.4790271169194966e-08\n",
      "Epoch 65/100, Loss: 2.742335603045376e-08\n",
      "Epoch 66/100, Loss: 4.7859565199814824e-08\n",
      "Epoch 67/100, Loss: 6.738961815244693e-07\n",
      "Epoch 68/100, Loss: 9.872943707023296e-08\n",
      "Epoch 69/100, Loss: 1.5208899029062195e-08\n",
      "Epoch 70/100, Loss: 1.4352170865095104e-06\n",
      "Epoch 71/100, Loss: 3.184844388215424e-08\n",
      "Epoch 72/100, Loss: 3.6283108784118667e-07\n",
      "Epoch 73/100, Loss: 9.794313626798612e-08\n",
      "Epoch 74/100, Loss: 7.433415305513336e-08\n",
      "Epoch 75/100, Loss: 8.088895953051178e-08\n",
      "Epoch 76/100, Loss: 5.2024550001306125e-08\n",
      "Epoch 77/100, Loss: 1.90175644121382e-08\n",
      "Epoch 78/100, Loss: 8.953925316745881e-07\n",
      "Epoch 79/100, Loss: 2.4588825908722356e-07\n",
      "Epoch 80/100, Loss: 1.6870738761554094e-07\n",
      "Epoch 81/100, Loss: 5.143211723179775e-08\n",
      "Epoch 82/100, Loss: 2.220464523361443e-08\n",
      "Epoch 83/100, Loss: 1.2860916740464745e-06\n",
      "Epoch 84/100, Loss: 1.8051432348897833e-08\n",
      "Epoch 85/100, Loss: 1.7145364381576655e-06\n",
      "Epoch 86/100, Loss: 1.6663665292071528e-07\n",
      "Epoch 87/100, Loss: 4.8375495680375025e-06\n",
      "Epoch 88/100, Loss: 1.4446207785567822e-08\n",
      "Epoch 89/100, Loss: 1.6211322062531508e-08\n",
      "Epoch 90/100, Loss: 2.0239399489696552e-08\n",
      "Epoch 91/100, Loss: 1.330810448507691e-07\n",
      "Epoch 92/100, Loss: 4.9690566328308705e-08\n",
      "Epoch 93/100, Loss: 6.598182977768374e-08\n",
      "Epoch 94/100, Loss: 1.626495205186984e-08\n",
      "Epoch 95/100, Loss: 1.9309590371108243e-08\n",
      "Epoch 96/100, Loss: 1.468689845296467e-07\n",
      "Epoch 97/100, Loss: 2.4447791346915437e-08\n",
      "Epoch 98/100, Loss: 1.0204153078063882e-08\n",
      "Epoch 99/100, Loss: 1.1234327246256726e-08\n",
      "Epoch 100/100, Loss: 4.416015642050297e-08\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "input_dim = 200   # number of EMG channels\n",
    "hidden_dim = 128\n",
    "batch_size = 256\n",
    "output_dim = Y.shape[1]   # number of Delta channels\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMModel(input_size=200, hidden_size=128, num_layers=2, output_size = 22)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X,Y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs,labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #print(f\"Output {outputs.shape},\\t Labels {labels.shape}\")\n",
    "        loss = criterion(outputs,labels.squeeze(dim=2))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f8ccd829-250e-43bb-a2d3-de0f7d72bff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X[\u001b[38;5;241m60000\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 2\u001b[0m out_test \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 20\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 20\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/rnn.py:871\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    869\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 871\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m    872\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    873\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "X[60000].shape\n",
    "out_test = model(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428d1a3-710b-46c6-9744-7cee16616d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de802a-3a55-4e74-a7de-3e8c2e9277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_inf_or_nan(tensor):\n",
    "    has_inf = torch.isinf(tensor).any()\n",
    "    has_nan = torch.isnan(tensor).any()\n",
    "    return has_inf or has_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c5634-d3a3-4fc3-8ed0-24c50eef595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_batch = X[:batch_size]  # Replace 'batch_size' with the actual size you want to test\n",
    "input_batch = input_batch.clone().detach().requires_grad_(True)\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Pass the input batch through the model\n",
    "    test_output = model(input_batch)\n",
    "\n",
    "# Print the output from the untrained model\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed91ac-c258-432f-aa46-da8c1288169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LSTMModel(input_size=200, hidden_size=128, num_layers=2, output_size = 22)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the input and output tensors\n",
    "input_tensor = torch.randn(1, 10, 200).to(device)\n",
    "#output_tensor = torch.randn(1, 22, 1).to(device)\n",
    "output_tensor = torch.randn(1, 22).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Calculate loss\n",
    "loss = torch.nn.functional.mse_loss(output, output_tensor)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d256bf2-d7d3-4bf0-94e3-860c303a5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─LSTM: 1-1                              301,056\n",
      "├─Linear: 1-2                            2,838\n",
      "=================================================================\n",
      "Total params: 303,894\n",
      "Trainable params: 303,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─LSTM: 1-1                              301,056\n",
       "├─Linear: 1-2                            2,838\n",
       "=================================================================\n",
       "Total params: 303,894\n",
       "Trainable params: 303,894\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semg-inference)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
