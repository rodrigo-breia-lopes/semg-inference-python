{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0638fda-b97b-4857-8d59-5ef5c7045728",
   "metadata": {},
   "source": [
    "# The Ninapro Dataset\n",
    "[Can be found here](https://ninapro.hevs.ch/)\n",
    "\n",
    "In this week we used the Ninapro dataset DB1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f2e4c-25e7-4feb-8d53-90a8dcf0d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "rel_dir = 'datasets/ninapro/db1/s1/S1_A1_E1.mat'\n",
    "file_path = os.path.join(root_dir, rel_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac3bc1-0db5-4a97-970f-e0d9a6d47448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat as ld\n",
    "\n",
    "mat_data = ld(rel_dir, appendmat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79f7d0-4928-4553-957d-2884bd37ac6d",
   "metadata": {},
   "source": [
    "## Available fields in the ninapro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b3f6d-2c88-4e18-aa6a-41a9021f1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(mat_data.keys())\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955d030-7091-423f-9555-3c04a041c09b",
   "metadata": {},
   "source": [
    "## Loading all the subjects' data into a 'struct'.\n",
    "This struct 'data', will have the following structure:\n",
    "\n",
    "Data is an array of n subjects\n",
    "each subject containing 3 experiments\n",
    "each experiment containing a dictionary (key-value pair)\n",
    "we'll load the sEMG signals from all the channels, and the cyberglove data, for the hand joint angle regression task.\n",
    "\n",
    "We also computed the angle differences between samples, to model the correlation between joint angle variation and the incoming EMG signal. This variation per sample is given by the following equation:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\delta_i\\left[n\\right] &= \\begin{cases}\n",
    "        0 &\\quad,\\text{if } n \\leq 1\\\\\n",
    "        g_i\\left[n+1\\right]-g_i\\left[n\\right]\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Where $i$ is a cyberglove joint ($i \\in \\left[1,22\\right], i \\in \\mathbb{R}$), $\\delta\\left[n\\right]$ is the angle variation (in $\\degree$) at sample $n$, $g\\left[n\\right]$ is the cyberglove's uncalibrated angle at sample $n$.\n",
    "\n",
    "## Normalize data\n",
    "The emg signal was normalized from each channel's relative minimum and max, to $\\left[0,1\\right]$\n",
    "while the glove angles were normalized, from $\\left[0,360\\right]$ to $\\left[0,1\\right]$, using:\n",
    "\n",
    "\n",
    "$$x' = \\frac{x - \\text{min}~x}{\\text{max}~x - \\text{min}~x}$$\n",
    "\n",
    "And 'delta', was normalized from $\\left[-360,360\\right]$ to $\\left[-1,1\\right]$, using:\n",
    "\n",
    "$$x'' = 2 \\frac{x - \\text{min}~x}{\\text{max}~x - \\text{min}~x}$$\n",
    "\n",
    "(source: https://stats.stackexchange.com/questions/178626/how-to-normalize-data-between-1-and-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d63ac0-6572-4a6f-b6b3-74405e3a2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "# Define the structure array to store the data\n",
    "num_subjects = 27\n",
    "data = []\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "\n",
    "def normalizer(data_array,min_val,max_val,type):\n",
    "    \n",
    "    match type:\n",
    "        case \"uni\": \n",
    "            return (data_array - min_val) / (max_val - min_val)            \n",
    "        case \"bi\":\n",
    "            return 2 * ( (data_array - min_val) / (max_val - min_val ) ) - 1\n",
    "        case __:\n",
    "            raise Exception(f\"{type} type normalization not yet implemented.\")\n",
    "\n",
    "# Loop through each subject\n",
    "    # Initialize lists to hold the experimental data\n",
    "\n",
    "emg_data = []\n",
    "delta_data = []\n",
    "\n",
    "for subj_idx in range(1, num_subjects + 1):\n",
    "    # Set up the subject directory name and path\n",
    "    subject_dir = f\"s{subj_idx}\"\n",
    "    subject_path = os.path.join(dataset_path, subject_dir)\n",
    "\n",
    "\n",
    "    # Loop through each experiment for the current subject\n",
    "    for exp_idx in range(1, 4):\n",
    "        # Set up the experiment file name and path\n",
    "        exp_name = f\"S{subj_idx}_A1_E{exp_idx}.mat\"\n",
    "        exp_path = os.path.join(subject_path, exp_name)\n",
    "\n",
    "        # Load the MATLAB file\n",
    "        mat_file = loadmat(exp_path)\n",
    "\n",
    "        # Extract the relevant information from the loaded dictionary\n",
    "        emg = mat_file['emg']\n",
    "        glove = mat_file['glove']\n",
    "        \n",
    "        # glove delta\n",
    "        delta = np.diff(glove, axis=0) # compute differences\n",
    "        delta = np.pad(delta, ((1, 0), (0, 0)), mode='constant') # first sample delta = 0\n",
    "\n",
    "        # normalize data\n",
    "        norm_emg = normalizer(emg, np.min(emg, axis = 0), np.max(emg, axis = 0) , \"uni\") # min-max channel, [0;1]\n",
    "        #norm_glove = normalizer(glove,0,360, \"uni\") # min-max channel, [0;1]\n",
    "        norm_delta = normalizer(delta,-360,360, \"bi\")\n",
    "\n",
    "        emg_data.append(norm_emg)\n",
    "        delta_data.append(norm_delta)\n",
    "        \n",
    "        # Append the list of experiments to the data list\n",
    "emg_data = np.array(emg_data, dtype=object)\n",
    "delta_data = np.array(delta_data, dtype=object)\n",
    "\n",
    "np.savez_compressed(os.path.join(dataset_path, \"db_1_data.npz\"), emg=emg_data, delta=delta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca55d6c-6e4f-4985-b649-30c2c2b516d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Next steps\n",
    "\n",
    "We now have the EMG signals to use as input to the network, and the dependent variable for regression, the $\\delta\\left[n\\right]$. This should suffice for the finger regression task, however, we'll\n",
    "try different approaches, such as:\n",
    "- Different network architectures: Vanilla RNN, LSTM, Bi-directional LSTM\n",
    "- Use of covariates, such as the physiological data of the subject (sex, age, height, weight, available in the ninapro dataset DB1), to try to\n",
    "improve inter-subject accuracy.\n",
    "- Use of amputated patient data from the Ninapro dataset.\n",
    "\n",
    "The next step is to break down the data into batches, and then try training a RNN with differnt input sequence lenghts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f6369-0622-4928-97fd-c252bd82faf6",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e8ec789e-d6f9-45da-bb83-014a68a81e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62721, 10, 200)\n",
      "(62721, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "window_size = 200 # splits all datapoints into 200 samples\n",
    "\n",
    "def transpose_array(array):\n",
    "    for idx in range(len(array)):\n",
    "        array[idx] = np.transpose(array[idx])\n",
    "    array_t = array\n",
    "    return array_t\n",
    "\n",
    "# load processed file\n",
    "# put samplesXchannel into channelXsamples\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "data_load_path = os.path.join(dataset_path,\"db_1_data.npz\")\n",
    "with np.load(data_load_path, allow_pickle=True) as data:\n",
    "    emg_data = transpose_array(data['emg']) \n",
    "    delta_data = transpose_array(data['delta'])\n",
    "\n",
    "def segment_data(emg_data, delta_data, window_size):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    num_trials = emg_data.shape[0]\n",
    "    for trial_idx in range(num_trials):\n",
    "        # Calculate the largest index that is a multiple of window_size\n",
    "        max_idx = (emg_data[trial_idx].shape[1] // window_size) * window_size\n",
    "        #print(f'Trial {trial_idx}: data size = {emg_data[trial_idx].shape[1]}, max_idx = {max_idx}')\n",
    "        for start_idx in range(0, max_idx, window_size):\n",
    "            input_t = emg_data[trial_idx][:,start_idx:start_idx + window_size]\n",
    "            target_t = delta_data[trial_idx][:,start_idx:start_idx + 1]\n",
    "            inputs.append(input_t)\n",
    "            targets.append(target_t)\n",
    "\n",
    "    inputs = np.stack(inputs, axis=0)\n",
    "    targets = np.stack(targets, axis=0)\n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "X, Y = segment_data(emg_data, delta_data, window_size)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "#dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "#filename = os.path.join(dataset_path,'segmented_data.npz')\n",
    "#np.savez_compressed(filename, inputs=X, targets=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de888714-f9b2-484a-ae1c-b1d8f74cd302",
   "metadata": {},
   "source": [
    "# TODO amanh√£\n",
    "\n",
    "Tentar treinar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83293837-51be-4083-92a5-3eb269d4960f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e0ccd50-d447-40d6-aedb-1ce0cfa045aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model properties\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Multivariate LSTM\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "471f08e0-ffde-4aaa-b9b9-bb9e654b11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset_path = os.path.join(\"datasets\", \"ninapro\", \"db1\")\n",
    "filename = os.path.join(dataset_path,'segmented_data.npz')\n",
    "\n",
    "with np.load(filename) as data:\n",
    "    X = data['inputs']\n",
    "    Y = data['targets']\n",
    "\n",
    "X = torch.from_numpy(X).to(torch.float32).to(device)\n",
    "Y = torch.from_numpy(Y).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f4d565c-81b1-446e-a152-cf28b24d6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X).to(torch.float32).to(device)\n",
    "Y = torch.from_numpy(Y).to(torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d99e46-d6ac-4895-84b0-55bb7c4d29b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.2983847454961506e-06\n",
      "Epoch 2/100, Loss: 1.075108571058081e-07\n",
      "Epoch 3/100, Loss: 1.2213524769322248e-07\n",
      "Epoch 4/100, Loss: 1.718315587595498e-07\n",
      "Epoch 5/100, Loss: 1.2356220224774006e-07\n",
      "Epoch 6/100, Loss: 2.265903447096207e-07\n",
      "Epoch 7/100, Loss: 8.636562824904104e-07\n",
      "Epoch 8/100, Loss: 1.1568371860448678e-07\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "input_dim = 200   # number of EMG channels\n",
    "hidden_dim = 128\n",
    "batch_size = 256\n",
    "output_dim = Y.shape[1]   # number of Delta channels\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMModel(input_size=200, hidden_size=128, num_layers=2, output_size = 22)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(X,Y)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs,labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #print(f\"Output {outputs.shape},\\t Labels {labels.shape}\")\n",
    "        loss = criterion(outputs,labels.squeeze(dim=2))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccd829-250e-43bb-a2d3-de0f7d72bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428d1a3-710b-46c6-9744-7cee16616d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de802a-3a55-4e74-a7de-3e8c2e9277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_inf_or_nan(tensor):\n",
    "    has_inf = torch.isinf(tensor).any()\n",
    "    has_nan = torch.isnan(tensor).any()\n",
    "    return has_inf or has_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c5634-d3a3-4fc3-8ed0-24c50eef595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_batch = X[:batch_size]  # Replace 'batch_size' with the actual size you want to test\n",
    "input_batch = input_batch.clone().detach().requires_grad_(True)\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Pass the input batch through the model\n",
    "    test_output = model(input_batch)\n",
    "\n",
    "# Print the output from the untrained model\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ed91ac-c258-432f-aa46-da8c1288169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = LSTMModel(input_size=200, hidden_size=128, num_layers=2, output_size = 22)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the input and output tensors\n",
    "input_tensor = torch.randn(1, 10, 200).to(device)\n",
    "#output_tensor = torch.randn(1, 22, 1).to(device)\n",
    "output_tensor = torch.randn(1, 22).to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(input_tensor)\n",
    "\n",
    "# Calculate loss\n",
    "loss = torch.nn.functional.mse_loss(output, output_tensor)\n",
    "\n",
    "# Backpropagation\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d256bf2-d7d3-4bf0-94e3-860c303a5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "‚îú‚îÄLSTM: 1-1                              301,056\n",
      "‚îú‚îÄLinear: 1-2                            2,838\n",
      "=================================================================\n",
      "Total params: 303,894\n",
      "Trainable params: 303,894\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "‚îú‚îÄLSTM: 1-1                              301,056\n",
       "‚îú‚îÄLinear: 1-2                            2,838\n",
       "=================================================================\n",
       "Total params: 303,894\n",
       "Trainable params: 303,894\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (semg-inference)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
